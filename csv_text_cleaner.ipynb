{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b668dfdf-8042-4ed3-af6b-a7f531a04d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8d54c145-ddd4-44f2-afe9-7212bf8daff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is E807-C1C7\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\n",
      "\n",
      "25/07/2023  02:07 pm    <DIR>          .\n",
      "25/07/2023  05:35 pm    <DIR>          ..\n",
      "25/07/2023  12:19 pm            22,396 csv_text_cleaner.ipynb\n",
      "25/07/2023  05:33 pm    <DIR>          datasets\n",
      "25/07/2023  05:36 pm    <DIR>          raw_csv\n",
      "24/07/2023  02:18 pm               104 README.md\n",
      "               2 File(s)         22,500 bytes\n",
      "               4 Dir(s)  162,756,947,968 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9061fde7-b324-45bd-aa4d-12759f64d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\n"
     ]
    }
   ],
   "source": [
    "#make sure directory is in csv_text_cleaner folder\n",
    "try:\n",
    "    dir = r\"C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\"\n",
    "    print(dir)\n",
    "    os.chdir(dir)\n",
    "    os.system(\"cd \"+dir)\n",
    "except OSError as e:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "72f408ea-0e65-4264-a4af-9b7385403f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatically detect csv's in raw_csv directory\n",
    "#isolate the csv files\n",
    "csv_dir = f\"{dir}\\\\raw_csv\\\\\"\n",
    "csv_files = []\n",
    "for file in os.listdir(csv_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1b23fe87-f968-4dbd-99c7-07985770a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new directory\n",
    "\n",
    "dataset_dir = r'C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\\datasets'\n",
    "\n",
    "#create the bash command to make a new directory\n",
    "# mkdir dataset_dir\n",
    "try:\n",
    "    mkdir = 'mkdir {0}'.format(dataset_dir)\n",
    "    os.system(mkdir)\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c6c71306-6b41-4547-bc59-32a73060c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world-data-2023.csv copied to C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\\datasets\n",
      "worldcities.csv copied to C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\\datasets\n",
      "world_data_2023.csv copied to C:\\Users\\Administrator\\Documents\\jupyter_projects\\csv_text_cleaner\\datasets\n"
     ]
    }
   ],
   "source": [
    "#copy the csv files to the new directory\n",
    "#copy <path+filename> <directory>\n",
    "for csv in csv_files:\n",
    "    copyfile = f\"copy {csv_dir}\\\"{csv}\\\" \\\"{dataset_dir}\\\\\\\"\"\n",
    "    os.system(copyfile)\n",
    "    print(f\"{csv} copied to {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "97c99e24-e040-46d1-9307-afd73b3b9691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world-data-2023.csv\n",
      "worldcities.csv\n",
      "world_data_2023.csv\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df[file] = pd.read_csv(f\"{dataset_dir}\\{file}\")\n",
    "    except UnicodeDecodeError:\n",
    "         df[file] = pd.read_csv(f\"{dataset_dir}\\{file}\", encoding=\"ISO-8859-1\")\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dacbc9ab-dfd6-485f-9a70-f0a154b7a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL!\n",
      "world_data_2023 table dropped if exists.\n",
      "world_data_2023 table created!\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table world_data_2023 import to database completed\n",
      "Connected to PostgreSQL!\n",
      "worldcities table dropped if exists.\n",
      "worldcities table created!\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table worldcities import to database completed\n",
      "Connected to PostgreSQL!\n",
      "world_data_2023 table dropped if exists.\n",
      "world_data_2023 table created!\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table world_data_2023 import to database completed\n",
      "all tables have been successfully imported\n"
     ]
    }
   ],
   "source": [
    "def text_cleaner(cleaned_string, units_to_remove=None):\n",
    "    if units_to_remove is None:\n",
    "        units_to_remove = []\n",
    "\n",
    "    replace_dict = {\n",
    "        '-': '_',\n",
    "        ' ':'_',\n",
    "        ':':'',\n",
    "        '&': '',\n",
    "        '$':'',\n",
    "        '%':'',\n",
    "        '!':'',\n",
    "        '/':'',\n",
    "        '(': '',\n",
    "        ')': '',\n",
    "        '\\\\': '',\n",
    "        '\\n': '',\n",
    "    }\n",
    "\n",
    "    #remove units from the cleaned name\n",
    "    for unit in units_to_remove:\n",
    "        cleaned_string = re.sub(re.escape(unit), '', cleaned_string)\n",
    "\n",
    "    #remove trailing symbols from the cleaned string\n",
    "    cleaned_string = re.sub(r'^[_-]+|[_-]+$','',cleaned_string)\n",
    "\n",
    "    #remove non-latin characters and transliterate with nearest equivalent\n",
    "    cleaned_string = unidecode(cleaned_string)\n",
    "    \n",
    "    cleaned_string = cleaned_string.rstrip().lower().translate(str.maketrans(replace_dict))\n",
    "\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "#replace pandas dataframe datatypes with equivalent SQL datatypes\n",
    "data_type_replacements = {\n",
    "    'object':'varchar',\n",
    "    'float64':'float',\n",
    "    'int64':'int',\n",
    "    'datetime64':'timestamp',\n",
    "    'timedelta64[ns]':'varchar'\n",
    "}\n",
    "\n",
    "#PostgreSQL credentials\n",
    "\n",
    "host = \"localhost\"\n",
    "database = \"postgres\"\n",
    "user = \"postgres\"\n",
    "password = \"password\"\n",
    "\n",
    "for k in csv_files:\n",
    "    dataframe = df[k]\n",
    "    \n",
    "    #insert csv file pathway into str\n",
    "    csv_path = str(k)\n",
    "    \n",
    "    #split file pathway to split file name into tuple, containing name( pos [0]) and file extension ( pos [1])\n",
    "    pathway = os.path.basename(k)\n",
    "    file_name = os.path.splitext(pathway)\n",
    "\n",
    "\n",
    "    #apply text_cleaner to title and headers\n",
    "    \n",
    "    #clean title\n",
    "    cleaned_title = text_cleaner(file_name[0])\n",
    "\n",
    "    #clean headers\n",
    "    dataframe.columns = [text_cleaner(text, units_to_remove=['pkm2','km2','co2']) for text in dataframe.columns]\n",
    "\n",
    "    #zip list of headers and corresponding datatypes so they are in format: '..., {header} {datatype}, ...'\n",
    "    col_str = \", \".join(\"{} {}\".format(n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(data_type_replacements)))\n",
    "\n",
    "    #establish a connection to the PostgreSQL server\n",
    "\n",
    "    try: \n",
    "        \n",
    "        connection = psycopg2.connect(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "        )\n",
    "    \n",
    "        print(\"Connected to PostgreSQL!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error connecting to PostgreSQL\", e)\n",
    "        exit()\n",
    "    \n",
    "    #set client encoding\n",
    "    connection.set_client_encoding('UTF8')\n",
    "    \n",
    "    \n",
    "    #create a cursor to execute SQL queries\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    #drop table with same name\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(f\"drop table if exists {cleaned_title} CASCADE;\")\n",
    "        print(f\"{cleaned_title} table dropped if exists.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error dropping table\", e)\n",
    "        exit()\n",
    "        \n",
    "    #create the table in PostgreSQL\n",
    "    \n",
    "    try:\n",
    "        create_table_query = f\"CREATE TABLE {cleaned_title} ({col_str});\"\n",
    "        cursor.execute(create_table_query)\n",
    "        print(f\"{cleaned_title} table created!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error creating table\", e)\n",
    "        exit()\n",
    "    \n",
    "    #insert the data into the table\n",
    "\n",
    "    #save df to csv\n",
    "\n",
    "    dataframe.to_csv(k, header=dataframe.columns, index=False, encoding='utf-8')\n",
    "    \n",
    "    #open file in memory\n",
    "    my_file = open(k, mode ='r', encoding='utf-8')\n",
    "    print(\"file opened in memory\")\n",
    "    \n",
    "    #upload to db\n",
    "    SQL_STATEMENT = f\"\"\"\n",
    "    COPY {cleaned_title} FROM STDIN WITH\n",
    "        CSV\n",
    "        HEADER\n",
    "        DELIMITER AS ','\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.copy_expert(sql=SQL_STATEMENT, file = my_file)\n",
    "    print(\"file copied to db\")\n",
    "\n",
    "    #change permissions to public\n",
    "    cursor.execute(f\"GRANT SELECT ON TABLE {cleaned_title} TO public\")\n",
    "    connection.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    print(f\"table {cleaned_title} import to database completed\")\n",
    "\n",
    "print(\"all tables have been successfully imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6186c65-fc51-486b-b08e-da7e1d132d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
